import json
import os
from pandas.io.json import json_normalize
import numpy as np
import pickle
from sklearn import svm
from sklearn.linear_model import SGDClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.neural_network import MLPClassifier



def load_2():
    # Loading Data from Files!
    training_folders = ['buy','fun','hope','really','communicate','mother']
    training_folder_location = '../training_data/csv/'
    raw_training_data_array = {}
    data_set_size = 0
    label_vector = numpy.array([])
    max_length = 0
    #training_features = ['rightWrist_y', 'rightWrist_x', 'leftWrist_y', 'leftWrist_x', 'leftElbow_x', 'leftElbow_y', 'rightElbow_x', 'rightElbow_y']
    training_features = ['rightWrist_y', 'rightWrist_x', 'leftWrist_y', 'leftWrist_x']


    # Takes 1 video data set and 1 data identifier and gives you a feature vector!
    def getFeatureVector(raw_data, training_feature):
        zeroCrossingArray = numpy.array([])
        maxDiffArray = numpy.array([])
        rY = raw_data[training_feature]
        normRawData = (rY - numpy.mean(rY))/(numpy.max(rY-numpy.mean(rY))-numpy.min(rY-numpy.mean(rY)))
        diffNormRawData = numpy.diff(normRawData)

        if diffNormRawData[0] > 0:
            initSign = 1
        else:
            initSign = 0

        windowSize = 5;

        for x in range(1, len(diffNormRawData)):
            if diffNormRawData[x] > 0:
                newSign = 1
            else:
                newSign = 0

            if initSign != newSign:
                zeroCrossingArray = numpy.append(zeroCrossingArray, x)
                initSign = newSign
                maxIndex = numpy.minimum(len(diffNormRawData),x+windowSize)
                minIndex = numpy.maximum(0,x - windowSize)
                maxVal = numpy.amax(diffNormRawData[minIndex:maxIndex])
                minVal = numpy.amin(diffNormRawData[minIndex:maxIndex])
                maxDiffArray = numpy.append(maxDiffArray, (maxVal - minVal))


        index = numpy.argsort(-maxDiffArray)

        feature_vector = numpy.array([])

        if (len(diffNormRawData) < (max_length - 1)):
            zeros = numpy.zeros(max_length - 1 - len(diffNormRawData))
            diffNormRawData = numpy.append(diffNormRawData, zeros)
        feature_vector = numpy.append(feature_vector, diffNormRawData)

        temp_array = zeroCrossingArray[index[0:5]]
        if (len(temp_array) < 5):
            zeros = numpy.zeros(5 - len(temp_array))
            temp_array = numpy.append(temp_array, zeros)
        feature_vector = numpy.append(feature_vector, temp_array)

        temp_array = maxDiffArray[index[0:5]]
        if (len(temp_array) < 5):
            zeros = numpy.zeros(5 - len(temp_array))
            temp_array = numpy.append(temp_array, zeros)
        feature_vector = numpy.append(feature_vector, temp_array)

        return (feature_vector)


    # Takes 1 video data and gives you total features Matrix!
    def getFeatureMatrix(raw_data):
        counter = 1
        feature_matrix = numpy.array([])
        for training_feature in training_features:
            feature_vector = getFeatureVector(raw_data, training_feature)
            feature_matrix = numpy.append(feature_matrix, feature_vector)

        return (feature_matrix)


    for training_label in training_folders:
        file_list = os.listdir(training_folder_location + training_label + "/")
        for file in file_list:
            data_set_size = data_set_size + 1
            raw_training_data_array[data_set_size] = pd.read_csv(training_folder_location + training_label + "/" + file)
            if (len(raw_training_data_array[data_set_size]) > max_length):
                max_length = len(raw_training_data_array[data_set_size])
            label_vector = numpy.append(label_vector, training_label)

    # Feature Matrix to Hold all data to train
    feature_matrices = numpy.array([])

    # Generate Feature Matrix for each file!
    for i in raw_training_data_array:
        raw_data = raw_training_data_array[i]
        feature_matrix = getFeatureMatrix(raw_data)
        if (i == 1):
            feature_matrices = [feature_matrix]
        else:
            feature_matrices = numpy.concatenate((feature_matrices, [feature_matrix]),axis=0)


    #print (feature_matrices[1][1])
    #print (label_vector)
    #print (len(feature_matrices))
    #print (len(label_vector))
    #print (len())
    #print(feature_matrices.shape)
    #print (max_length)

    filename = '../models/model_1.pkl'
    clf = svm.SVC(kernel='linear',gamma='scale')
    clf = clf.fit(feature_matrices, label_vector)
    pickle.dump(clf, open(filename, 'wb'))

    filename = '../models/model_2.pkl'
    #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1,max_iter=2000)
    clf = SGDClassifier(loss='perceptron',random_state=5)
    clf = clf.fit(feature_matrices, label_vector)
    pickle.dump(clf, open(filename, 'wb'))

    filename = '../models/model_2.pkl'
    #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1,max_iter=2000)
    #clf = SGDClassifier(loss='perceptron',random_state=5)
    #clf = clf.fit(feature_matrices, label_vector)
    #pickle.dump(clf, open(filename, 'wb'))

    filename = '../models/model_4.pkl'
    clf = LogisticRegression(random_state=1, solver='lbfgs',multi_class='multinomial',max_iter=2000)
    clf = clf.fit(feature_matrices, label_vector)
    pickle.dump(clf, open(filename, 'wb'))

def load_1():
    max_data_size = 0
    all_data = []
    training_folders = ['buy','fun','hope','really','communicate','mother']
    training_folder_location = '../training_data/json/'
    all_data = []
    all_data_labels = []
    traning_data = []
    test_data= []
    training_files =[]
    data_lengths = []

    for training_label in training_folders:
        file_list = os.listdir(training_folder_location + training_label + "/")
        for file in file_list:
            with open(training_folder_location + training_label + "/" + file) as video_entry:
                training_entry = json.load(video_entry)

            result = json_normalize(training_entry, 'keypoints', ['score'], record_prefix='keypoints.')
            #result = result.reindex(columns=['score', 'keypoints.part', 'keypoints.score', 'keypoints.position.x', 'keypoints.position.y']) # Take away part pt 2
            result = result.reindex(columns=['score', 'keypoints.score', 'keypoints.position.x', 'keypoints.position.y'])
            result = np.array(result)
            #result = np.reshape(result, (-1, 85)) # Take away part pt 2
            result = np.reshape(result, (-1, 68))
            result = result[:, 12:44] # Narrow features to ignore hips and some facial features
            result = list(result.ravel())
            all_data.append(result)
            data_lengths.append(len(result))
            all_data_labels.append(training_label)
    

    # 0 Padding
    max_data_size = max(data_lengths)
    for item in all_data:
        item.extend([0] * (max_data_size - len(item)))
        
    print(max_data_size)

    #filename = '../models/model_1.pkl'
    #clf = svm.SVC(gamma='auto')
    #clf = clf.fit(all_data, all_data_labels)
    #pickle.dump(clf, open(filename, 'wb'))

    filename = '../models/model_2.pkl'
    #clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5,2), random_state=1,max_iter=1000)
    clf = SGDClassifier(loss='perceptron',random_state=5)
    clf = clf.fit(all_data, all_data_labels)
    pickle.dump(clf, open(filename, 'wb'))

    #filename = '../models/model_3.pkl'
    #clf = RandomForestClassifier(n_estimators=800, max_depth=2, random_state=3)
    #clf = clf.fit(all_data, all_data_labels)
    #pickle.dump(clf, open(filename, 'wb'))

    #filename = '../models/model_4.pkl'
    #clf = LogisticRegression(random_state=1, solver='lbfgs',multi_class='multinomial',max_iter=4000)
    #clf = clf.fit(all_data, all_data_labels)
    #pickle.dump(clf, open(filename, 'wb'))


load_1()
load_2()





